# Auditoria Completa ‚Äî Redu√ß√£o de Custos Cloud Storage + App Engine / Cloud Functions

> **Data:** 15/02/2026  
> **Projeto:** Partiu (partiu-479902)  
> **Escopo:** Cloud Storage, Cloud Functions (Firebase), Cloud Run, Firestore (impacto indireto em App Engine billing)  
> **Status:** Diagn√≥stico completo com recomenda√ß√µes priorizadas

---

## Sum√°rio Executivo

A infraestrutura do Partiu **n√£o possui App Engine expl√≠cito** ‚Äî o custo rotulado como "App Engine" no billing do GCP corresponde provavelmente a:

1. **Cloud Functions v1** (63 fun√ß√µes deployadas) ‚Äî executam na infraestrutura App Engine internamente
2. **Inst√¢ncia default do App Engine** criada automaticamente pelo GCP ao ativar Cloud Functions v1
3. **Cloud Run** (`partiu-websocket`) ‚Äî servi√ßo NestJS WebSocket com 1 CPU / 512Mi

> **Importante:** Cloud Functions v1 do Firebase s√£o executadas internamente no App Engine Flex/Standard. No billing do GCP, elas aparecem como custo de "App Engine" e n√£o como "Cloud Functions". Isso explica por que "App Engine" √© o maior custo mesmo sem app.yaml no reposit√≥rio.

### Resumo de Impacto Estimado

| Categoria | Economia Estimada | Esfor√ßo | Prioridade |
|-----------|-------------------|---------|------------|
| Eliminar fun√ß√µes desnecess√°rias (11 migrations/debug) | 5-8% do custo Functions | Baixo | üî¥ Imediata |
| Otimizar cron jobs (reduzir frequ√™ncia) | 10-15% do custo Functions | Baixo | üî¥ Imediata |
| Corrigir listeners Firestore no client (custo indireto) | 20-30% das leituras | M√©dio | üî¥ Alta |
| Eliminar streams sem `.limit()` | 15-20% das leituras | M√©dio | üî¥ Alta |
| Otimizar Feed Fanout (N writes per follower) | 10-15% das escritas | Alto | üü° M√©dia |
| Gerar thumbnails server-side (reduz egress Storage) | 15-25% do egress | M√©dio | üü° M√©dia |
| Migrar para Cloud Functions v2 (gen2) | 15-30% do custo Functions | M√©dio | üü° M√©dia |
| Migrar para API pr√≥pria (PostgreSQL) | 85-90% do Firestore | Muito Alto | üü¢ Longo prazo |

---

## Parte 1 ‚Äî App Engine / Cloud Functions: Diagn√≥stico Detalhado

### 1.1 Por que "App Engine" aparece como maior custo

Cloud Functions v1 do Firebase s√£o **executadas internamente na infraestrutura App Engine**. No console de billing do GCP:

- **Compute** das Cloud Functions ‚Üí aparece como "App Engine"
- **Invoca√ß√µes** ‚Üí aparecem como "Cloud Functions"
- **Mem√≥ria/CPU** alocada por fun√ß√£o ‚Üí contabilizada como App Engine instance hours

**O projeto tem 63 Cloud Functions deployadas**, sendo que pelo menos **11 s√£o migrations/debug que deveriam ser removidas** e **9 cron jobs** que executam em intervalos variados (5 min a di√°rio).

### 1.2 Fun√ß√µes para Remo√ß√£o Imediata (custo zero √∫til)

Estas fun√ß√µes s√£o de migra√ß√£o/debug e **n√£o deveriam estar deployadas em produ√ß√£o**:

| Fun√ß√£o | Tipo | Por que remover |
|--------|------|-----------------|
| `backfillUserGeohash` | HTTP | Migra√ß√£o pontual j√° executada |
| `backfillEventCreatorData` | HTTP | Migra√ß√£o pontual j√° executada |
| `backfillEventPreviewsLocation` | HTTP | Migra√ß√£o pontual j√° executada |
| `backfillEventPreviewsCategory` | HTTP | Migra√ß√£o pontual j√° executada |
| `backfillMissingNotificationTimestamps` | Cron (2h) | Backfill ‚Äî deveria ser one-shot |
| `migrateUserLocationToPrivate` | HTTP | Migra√ß√£o pontual j√° executada |
| `resyncUsersPreview` | HTTP | Manuten√ß√£o pontual |
| `patchAddCountryFlag` | HTTP | Patch pontual j√° executado |
| `patchRemoveFormattedAddress` | HTTP | Patch pontual j√° executado |
| `debugCreateNotification` | HTTP | Debug ‚Äî nunca deveria estar em produ√ß√£o |
| `backfillMissingNotificationTimestamps` | Cron | Corre√ß√£o que roda a cada 2h desnecessariamente |

**A√ß√£o:** Remover exports do `index.ts` e re-deploy. Economia estimada: **~$5-15/m√™s** em instance hours ociosas + elimina√ß√£o de execu√ß√µes desnecess√°rias do cron.

### 1.3 Cron Jobs ‚Äî Otimiza√ß√£o de Frequ√™ncia

| Job | Frequ√™ncia Atual | Frequ√™ncia Recomendada | Economia |
|-----|------------------|------------------------|----------|
| `processEventDeletions` | **A cada 5 min** | A cada 1 hora ou Firestore trigger | **~288 ‚Üí 24 execu√ß√µes/dia** (92% menos) |
| `syncRankingFilters` | **A cada 30 min** | 1x/dia (03:00) | **~48 ‚Üí 1 execu√ß√£o/dia** (98% menos) |
| `processProfileViewNotifications` | A cada 15 min | A cada 1 hora | **~96 ‚Üí 24 execu√ß√µes/dia** (75% menos) |
| `createPendingReviewsScheduled` | A cada 1 hora | A cada 6 horas | **~24 ‚Üí 4 execu√ß√µes/dia** (83% menos) |

**Detalhamento de `syncRankingFilters`:**
- L√™ a **cole√ß√£o `users_preview` INTEIRA** (paginada em blocos de 500)
- Com 10.000 usu√°rios = ~20 reads de p√°gina + 10.000 reads de documentos = **~10.020 leituras por execu√ß√£o**
- A cada 30 min = **~480.960 leituras/dia** s√≥ para extrair `DISTINCT state, city`
- Configura√ß√£o: 512MB mem√≥ria, 540s timeout
- **Solu√ß√£o:** Reduzir para 1x/dia ‚Üí ~10.020 leituras/dia (economia de ~470k leituras/dia)

**Detalhamento de `processEventDeletions`:**
- Roda a cada 5 minutos e l√™ todos os eventos marcados como "pending deletion"
- Multi-fase: messages ‚Üí applications ‚Üí notifications ‚Üí feedItems ‚Üí finalize
- **Solu√ß√£o:** Trocar cron de 5 min por Firestore trigger `onUpdate` no campo `deletionStatus`

### 1.4 Cloud Functions com Custo Operacional Alto

#### üî¥ `onActivityCreatedNotification` ‚Äî Geo-query + N writes por evento

**O que faz:** Quando um evento √© criado, busca todos os usu√°rios num raio geogr√°fico e cria 1 notifica√ß√£o por usu√°rio.

**Custo por execu√ß√£o:**
- 1 read do criador (`Users`)
- 1 query geogr√°fica na cole√ß√£o `Users` (at√© 500 docs)
- **At√© 500 writes** na cole√ß√£o `Notifications`
- Total: **~502 opera√ß√µes Firestore por evento criado**

**Recomenda√ß√£o:**
- Limitar resultados do geo-query (`.limit(100)` em vez de 500)
- Usar `users_preview` em vez de `Users` (documento menor = menor egress)
- Considerar batch notification com document-array em vez de 1 doc por notifica√ß√£o

#### üî¥ `onApplicationApproved` ‚Äî N+1 reads por aprova√ß√£o

**O que faz:** Quando uma application √© aprovada, busca dados do evento, do usu√°rio, de TODAS as applications aprovadas, e depois busca o doc completo `Users` de **cada participante individualmente**.

**Custo por execu√ß√£o (evento com 15 participantes):**
- 2 reads paralelas (`events` + `Users`)
- 1 query em `EventApplications` (~15 docs)
- **15 reads individuais** em `Users` (N+1 pattern)
- 15+ writes em `Connections/Conversations`
- 1 write de mensagem em `EventChats/Messages`
- Total: **~50 opera√ß√µes por aprova√ß√£o**

**Recomenda√ß√£o:**
- Usar `users_preview` em vez de `Users` completo
- Usar `getAll()` (batch read) em vez de `Promise.all(map(doc.get()))` individual
- Guardar dados m√≠nimos dos participantes no `EventChat.participantIds` para evitar re-fetch

#### üî¥ Feed Fanout System ‚Äî N writes por post

**O que faz:** Para cada foto/atividade postada, cria 1 entrada no feed de **cada seguidor**.

**Custo por execu√ß√£o:**
- `onEventPhotoWriteFanout`: at√© **5.000 writes** (1 por seguidor)
- `onActivityFeedWriteFanout`: at√© **5.000 writes** (1 por seguidor)
- `onNewFollowerBackfillFeed`: l√™ √∫ltimos 20 EventPhotos + 20 ActivityFeed do autor, escreve tudo no feed do novo seguidor
- `onUnfollowCleanupFeed`: query all items by author + delete in batches

**Recomenda√ß√£o:**
- Migrar para **pull model** (o feed √© montado no momento da leitura via query, n√£o por fan-out de escrita)
- Alternativa: limitar fan-out para os primeiros 500 seguidores mais recentes

#### üü° `onEventWriteUpdateCardPreview` ‚Äî Denormaliza√ß√£o por write

**O que faz:** A cada write na cole√ß√£o `events`, l√™ o criador de `Users` e escreve/atualiza `events_card_preview`.

**Custo:** 2 opera√ß√µes Firestore extras **por cada update no evento**.

**Recomenda√ß√£o:** Eliminar `events_card_preview` e fazer JOIN no client (ou migrar para API com PostgreSQL).

#### üü° `onUserProfileUpdateSyncEvents` ‚Äî Cascata de updates

**O que faz:** Quando o perfil do usu√°rio muda, l√™ TODOS os `events_card_preview` daquele usu√°rio e batch-update em todos.

**Custo:** Se um usu√°rio criou 50 eventos ‚Üí 51 reads + 50 writes por update de perfil.

#### üü° `updateLocationRanking` ‚Äî Query unbounded por evento

**O que faz:** Na cria√ß√£o de evento, faz query de TODOS os eventos ativos no mesmo `placeId` + reads individuais de usu√°rios.

**Recomenda√ß√£o:** Usar counter incremental em vez de reagregar tudo a cada evento.

### 1.5 Migra√ß√£o para Cloud Functions v2 (gen2)

Cloud Functions v2 usa **Cloud Run** internamente (n√£o App Engine), com billing diferente:

| Aspecto | v1 (atual) | v2 (gen2) |
|---------|-----------|-----------|
| Runtime | App Engine | Cloud Run |
| Billing | Instance-hours (idle cobra) | Por request (concurrency-aware) |
| Concurrency | 1 request por inst√¢ncia | At√© 1000 por inst√¢ncia |
| Cold starts | ~500ms-2s | ~200ms-1s |
| Min instances | N√£o configur√°vel facilmente | Configur√°vel (0 = sem custo idle) |
| Custo t√≠pico | Mais caro para fun√ß√µes leves | **30-50% mais barato** |

**A√ß√£o recomendada:** Migrar as fun√ß√µes mais invocadas (`getPeople`, `onEventChatMessageCreated`, `activityPushNotifications`) para v2 primeiro.

```typescript
// v1 (atual)
import * as functions from "firebase-functions/v1";
export const getPeople = functions.https.onCall(async (data, context) => { ... });

// v2 (recomendado)
import { onCall } from "firebase-functions/v2/https";
export const getPeople = onCall({ 
  region: "southamerica-east1", // S√£o Paulo
  memory: "256MiB",
  concurrency: 80,
  minInstances: 0, // Zero custo idle
}, async (request) => { ... });
```

---

## Parte 2 ‚Äî Cloud Storage: Diagn√≥stico e Otimiza√ß√µes

### 2.1 Estrutura Atual de Storage

| Path | Max Size | Uso |
|------|----------|-----|
| `users/{uid}/profile/**` | 15 MB | Avatares de perfil |
| `users/{uid}/gallery/**` | 15 MB | Galeria do usu√°rio |
| `users/{uid}/videos/**` | 200 MB | V√≠deos do usu√°rio |
| `events/{eventId}/cover/**` | 15 MB | Capas de eventos |
| `events/{eventId}/photos/{uid}/**` | 10 MB | Fotos de participantes |
| `event_photos/{eventId}/{fileName}` | 15 MB | Event Photo Feed |
| `messages/{uid}/**` | 15 MB | Arquivos de chat |
| `chat_images/**` | 15 MB | Imagens de chat |

### 2.2 Compress√£o Client-Side (j√° implementada)

| Contexto | Dimens√£o M√°x | Qualidade | Observa√ß√£o |
|----------|--------------|-----------|------------|
| Picker geral | 1920√ó1920 | 85 | ‚úÖ Bom |
| Avatar | 800√ó800 | 80 | ‚úÖ Bom |
| Event Photo Feed | 1080√ó1080 | 82 | ‚úÖ Bom |
| Thumb Event Photo | 420√ó420 | 70 | ‚úÖ Bom |
| Chat images | 1080√ó1080 | 75 | ‚úÖ Bom |
| Gallery (upload) | 1080√ó1080 | 75 | ‚ö†Ô∏è Picker em 1920 depois comprime para 1080 ‚Äî etapa desnecess√°ria |

### 2.3 Problemas Identificados em Cloud Storage

#### üî¥ PROBLEMA 1: Sem Thumbnails Server-Side

Apenas o Event Photo Feed gera thumbnail client-side (420px). **Perfil, galeria e chat n√£o t√™m thumbnails.**

**Impacto:**
- Listas que mostram avatares/previews baixam a imagem completa (1080px, ~200-500KB)
- Em uma lista de 50 usu√°rios, s√£o **~25MB de download** quando 50 thumbnails de 100px seriam ~500KB
- **Egress de Storage √© o principal custo de Cloud Storage**

**Recomenda√ß√£o:**
- Instalar a extens√£o Firebase **"Resize Images"** (`storage-resize-images`)
- Configurar para gerar thumbnails autom√°ticos: 150px (avatar list), 400px (preview), original
- Path: `users/{uid}/profile/` ‚Üí gera `thumb_150x150_`, `thumb_400x400_`
- Custo da extens√£o: ~$0.01 por 1000 imagens processadas (Cloud Functions)
- **Economia estimada: 60-80% do egress de Storage para imagens de perfil**

#### üî¥ PROBLEMA 2: `AvatarStore` usa `NetworkImage` (sem cache em disco)

**Arquivo:** `lib/shared/stores/avatar_store.dart`

```dart
final provider = NetworkImage(imageUrl);
```

`NetworkImage` usa apenas o `ImageCache` do Flutter (cache em mem√≥ria) que √© **vol√°til** ‚Äî imagens s√£o re-baixadas quando o cache de mem√≥ria √© limpo (troca de tab, scroll longo, etc.).

**Impacto:** Cada vez que um avatar some do cache de mem√≥ria, ele √© baixado novamente do Cloud Storage (egress pago).

**Recomenda√ß√£o:** Migrar para `CachedNetworkImageProvider` (mesmo que o app j√° use em `StableAvatar`):

```dart
// ‚ùå Atual:
final provider = NetworkImage(imageUrl);

// ‚úÖ Recomendado:
final provider = CachedNetworkImageProvider(imageUrl, cacheManager: avatarCacheManager);
```

#### üü° PROBLEMA 3: Coment√°rios do Feed usam `NetworkImage`

Imagens de comentaristas em Event Photo Feed usam `NetworkImage` direto ‚Äî sem cache em disco.
- Cada scroll no feed re-baixa os avatares dos comentaristas
- volume menor que AvatarStore, mas ainda gera egress desnecess√°rio

#### üü° PROBLEMA 4: Compress√£o dupla em Gallery Upload

O picker captura em 1920√ó1920/q85, e depois o ViewModel comprime para 1080√ó1080/q75.

**Recomenda√ß√£o:** Configurar o picker diretamente para 1080√ó1080 (elimina processamento intermedi√°rio e uso de mem√≥ria).

#### üü° PROBLEMA 5: Sem lifecycle policies no Cloud Storage

N√£o h√° lifecycle rules configuradas para:
- Deletar automaticamente imagens de eventos expirados
- Mover imagens antigas para Nearline/Coldline storage
- Limpar uploads √≥rf√£os (usu√°rios deletados)

**Recomenda√ß√£o:**
```json
// gsutil lifecycle set:
{
  "rule": [
    {
      "action": {"type": "SetStorageClass", "storageClass": "NEARLINE"},
      "condition": {"age": 180, "matchesPrefix": ["events/"]}
    },
    {
      "action": {"type": "Delete"},
      "condition": {"age": 365, "matchesPrefix": ["events/"]}
    }
  ]
}
```

### 2.4 CDN / Cache Headers

**Verificar se os objetos do Cloud Storage est√£o sendo servidos com headers de cache adequados:**

```bash
# Verificar headers atuais
gsutil stat gs://partiu-479902.appspot.com/users/test/profile/photo.jpg

# Configurar cache para avatares (cache p√∫blico de 24h)
gsutil -m setmeta -h "Cache-Control:public, max-age=86400" gs://partiu-479902.appspot.com/users/**

# Configurar cache para event photos (cache p√∫blico de 7 dias)
gsutil -m setmeta -h "Cache-Control:public, max-age=604800" gs://partiu-479902.appspot.com/events/**
```

**Se os headers n√£o estiverem configurados, cada acesso a `getDownloadURL()` paga egress sem cache intermedi√°rio.**

---

## Parte 3 ‚Äî Firestore (Client-Side): Maiores Gargalos de Custo

O custo de Firestore √© **leitura-dominante** e impacta indiretamente o billing de App Engine (via Cloud Functions triggers).

### 3.1 Top 10 Problemas de Custo no Client Flutter

#### üî¥ #1 ‚Äî Mapa carrega TODOS os eventos do mundo

**Arquivo:** `lib/features/home/data/repositories/event_map_repository.dart`

```dart
.collection('events')
.where('isActive', isEqualTo: true)
.where('status', isEqualTo: 'active')
.snapshots(includeMetadataChanges: true) // TODOS os eventos, sem filtro geo, sem limit
```

**Impacto:**
- Com 1.000 eventos ativos ‚Üí 1.000 reads no primeiro load + websocket cont√≠nuo
- `includeMetadataChanges: true` ‚Üí **dobra o n√∫mero de snapshots** (local + server)
- Cada novo evento criado por qualquer usu√°rio ‚Üí snapshot para TODOS os clients conectados
- **Estimativa: 50-70% de todas as leituras Firestore da aplica√ß√£o**

**Recomenda√ß√£o:**
- Implementar filtro por geohash prefix (j√° existem √≠ndices)
- Adicionar `.limit(100)` ou `.limit(200)`
- Remover `includeMetadataChanges: true`
- Usar debounce de viewport para queries por bounds

#### üî¥ #2 ‚Äî 3 listeners por Event Card vis√≠vel

**Arquivo:** `lib/features/home/presentation/widgets/event_card/event_card_controller.dart`

Cada card aberto cria 3 streams Firestore:
1. `EventApplications` (application do usu√°rio) ‚Äî `.limit(1)` ‚úÖ  
2. `events/{eventId}` (documento do evento) ‚Äî doc listener
3. `EventApplications` approved (participantes) ‚Äî **SEM `.limit()`** ‚ùå

**Impacto:** 10 cards vis√≠veis = **30 listeners simult√¢neos** + getter `participantsCountStream` pode criar duplicata = **40 listeners**.

**Recomenda√ß√£o:**
- Substituir listener #3 por counter field no documento do evento (`participantCount`)
- Substituir listener #2 por dados j√° carregados do mapa (n√£o precisa ser real-time)
- Resultado: **de 30-40 listeners para 10** (s√≥ application status)

#### üî¥ #3 ‚Äî AvatarStore + UserStore = 2-3 listeners por usu√°rio

**Arquivos:** `lib/shared/stores/avatar_store.dart` + `lib/shared/stores/user_store.dart`

Para cada usu√°rio visto no app:
- `AvatarStore` ‚Üí 1 listener em `Users/{uid}.snapshots()` (permanente)
- `UserStore` ‚Üí 1 listener em `users_preview/{uid}.snapshots()` (permanente)
- `UserStore` (full) ‚Üí 1 listener adicional em `Users/{uid}.snapshots()`

**Impacto:** Ap√≥s navigar por 50 perfis ‚Üí **100-150 listeners Firestore simult√¢neos** que nunca s√£o cancelados.

**Recomenda√ß√£o:**
- Substituir `.snapshots()` por `.get()` + cache TTL (10 min) no `UserCacheService`
- Manter listener APENAS para o usu√°rio logado
- Implementar LRU cache com hard cap (ex: m√°x 30 listeners ativos)

#### üî¥ #4 ‚Äî Counter Service streams 1000 docs para contar um inteiro

**Arquivo:** `lib/common/services/notifications_counter_service.dart`

```dart
// Stream 1: 1000 docs de Conversations para contar unread
.limit(1000).snapshots()

// Stream 2: 1000 docs de Notifications para contar n_read==false  
.limit(1000).snapshots()
```

**Impacto:** Transmite at√© **2.000 documentos completos** a cada mudan√ßa, apenas para contar badges.

**Recomenda√ß√£o:**
- Usar **Firestore Aggregation Query** (`count()`) ‚Äî dispon√≠vel desde 2023
- Ou manter counter field at√¥mico com `FieldValue.increment(1)` nas Cloud Functions
- Resultado: de 2.000 reads para **2 reads** por atualiza√ß√£o

#### üî¥ #5 ‚Äî Notifications stream sem `.limit()`

**Arquivo:** `lib/features/notifications/repositories/notifications_repository.dart`

```dart
query.orderBy(_fieldTimestamp, descending: true).snapshots() // SEM LIMIT
```

**Impacto:** Usu√°rio com 500 notifica√ß√µes ‚Üí 500 docs transmitidos a cada nova notifica√ß√£o (stream atualiza com snapshot completo).

**Recomenda√ß√£o:** Trocar para `getNotificationsPaginatedStream()` que j√° existe com `.limit(20)`.

#### üü° #6 ‚Äî Block Service: 2 streams sem limit

**Arquivo:** `lib/core/services/block_service.dart`

2 streams permanentes sem `.limit()` na cole√ß√£o `blockedUsers`. Cresce linearmente com n√∫mero de blocks.

#### üü° #7 ‚Äî List Drawer: stream de todos os eventos do criador

**Arquivo:** `lib/features/home/presentation/widgets/list_drawer/list_drawer_controller.dart`

```dart
.collection('events')
.where('createdBy', isEqualTo: userId)
.orderBy('createdAt', descending: true)
.snapshots() // SEM LIMIT ‚Äî criador prol√≠fico com 200+ eventos = 200 docs
```

**Recomenda√ß√£o:** Adicionar `.limit(20)` com pagina√ß√£o sob demanda.

#### üü° #8 ‚Äî Pending Applications: nested streams (N+1)

**Arquivo:** `lib/features/home/data/repositories/pending_applications_repository.dart`

Stream dentro de stream: outer stream sem limit cancela/recria inner stream a cada emiss√£o.

#### üü° #9 ‚Äî Profile Completeness como stream

**Arquivo:** `lib/features/profile/data/services/profile_completeness_prompt_service.dart`

Listener permanente em `Users/{uid}` para calcular % de completude ‚Äî muda 1x/semana. Deveria ser `.get()`.

#### üü¢ #10 ‚Äî `collectionGroup('likes')` (controlado)

Tem `.limit()` e √© one-time read. Baixo custo, mas monitorar em escala.

### 3.2 √çndices Firestore ‚Äî Inconsist√™ncias

| Problema | Impacto |
|----------|---------|
| Cole√ß√£o `Events` (mai√∫sculo) vs `events` (min√∫sculo) ‚Äî ambas com √≠ndices | Poss√≠vel duplica√ß√£o de cole√ß√µes ou √≠ndices ociosos |
| Chat collections sem nenhum √≠ndice composto | Queries compostas no chat fazem full collection scan |
| `Notifications` sem √≠ndice `userId + n_read` | Query de "n√£o lidos" faz full scan com filtro client-side |
| `users_preview` sem √≠ndice de geohash | Geo-queries em users fazem range scan por latitude |

---

## Parte 4 ‚Äî Cloud Run (`partiu-websocket`): Diagn√≥stico

### 4.1 Configura√ß√£o Atual

| Detalhe | Valor |
|---------|-------|
| Servi√ßo | `partiu-websocket` |
| Framework | NestJS 11 + Socket.IO |
| CPU | 1 |
| Mem√≥ria | 512Mi |
| Regi√£o | `us-central1` (longe dos usu√°rios BR) |
| Auth | `--allow-unauthenticated` |
| Timeout | 300s |
| Projeto Firebase | `partiu-app` (**diferente do Flutter: `partiu-479902`**) |

### 4.2 Problemas Identificados

#### üî¥ Regi√£o errada
Servi√ßo em `us-central1` mas usu√°rios est√£o no Brasil. Lat√™ncia adicional de ~150ms por request.

**Recomenda√ß√£o:** Migrar para `southamerica-east1` (S√£o Paulo).

#### üü° Poss√≠vel inst√¢ncia ociosa
WebSocket connections mant√™m inst√¢ncia ativa. Se poucos usu√°rios usam WebSocket, a inst√¢ncia pode ficar ociosa pagando CPU/mem√≥ria.

**Verificar no GCP Console:**
- M√©tricas de conex√µes ativas por hora
- Instance count vs requests/connections
- Se < 10 conex√µes/hora ‚Üí considerar desligar e usar polling HTTP

#### üü° Projeto Firebase diferente
Cloud Run aponta para `partiu-app` enquanto o Flutter usa `partiu-479902`. Verificar se s√£o o mesmo projeto ou se h√° duplica√ß√£o de custos Firestore.

---

## Parte 5 ‚Äî Plano de A√ß√£o Priorizado

### Fase 1 ‚Äî Quick Wins (1-2 dias, economia imediata)

| # | A√ß√£o | Economia Estimada | Arquivo(s) |
|---|------|-------------------|------------|
| 1.1 | Remover 11 functions de migration/debug do deploy | $5-15/m√™s | `functions/src/index.ts` |
| 1.2 | Reduzir `syncRankingFilters` para 1x/dia | ~470k reads/dia | `functions/src/ranking/rankingFiltersSync.ts` |
| 1.3 | Reduzir `processEventDeletions` para 1x/hora | ~276 execu√ß√µes/dia a menos | `functions/src/events/processEventDeletions.ts` |
| 1.4 | Remover `includeMetadataChanges: true` do mapa | ~50% dos snapshots do mapa | `lib/features/home/data/repositories/event_map_repository.dart` |
| 1.5 | Adicionar `.limit()` nos 5 streams sem limit | Reduz payload de cada snapshot | V√°rios (listados na Parte 3) |
| 1.6 | Configurar Cache-Control headers no Storage | Reduz egress em revisitas | `gsutil` command |

### Fase 2 ‚Äî Otimiza√ß√µes Importantes (1-2 semanas)

| # | A√ß√£o | Economia Estimada | Arquivo(s) |
|---|------|-------------------|------------|
| 2.1 | Substituir `NetworkImage` por `CachedNetworkImageProvider` no AvatarStore | 30-50% do egress de avatares | `lib/shared/stores/avatar_store.dart` |
| 2.2 | Converter AvatarStore/UserStore de `.snapshots()` para `.get()` + cache | 100+ listeners permanentes eliminados | `avatar_store.dart`, `user_store.dart` |
| 2.3 | Substituir counter service streams por aggregation query ou counter field | 2.000 ‚Üí 2 reads por badge update | `notifications_counter_service.dart` |
| 2.4 | Reduzir listeners do Event Card de 3 para 1 | 20+ listeners eliminados na tela principal | `event_card_controller.dart` |
| 2.5 | Instalar extens√£o Firebase "Resize Images" | 60-80% do egress de perfil | Firebase Console |
| 2.6 | Otimizar `onApplicationApproved` (batch read + users_preview) | 50% das reads por aprova√ß√£o | `functions/src/index.ts` |
| 2.7 | Corrigir `onActivityCreatedNotification` (limitar geo-query) | 500 ‚Üí 100 writes por evento | `functions/src/activityNotifications.ts` |
| 2.8 | Mover Cloud Run para `southamerica-east1` | Lat√™ncia -150ms | `wedding-websocket/cloudbuild.yaml` |

### Fase 3 ‚Äî Refatora√ß√µes Estruturais (1-2 meses)

| # | A√ß√£o | Economia Estimada | Complexidade |
|---|------|-------------------|--------------|
| 3.1 | Migrar mapa para query por geohash/viewport | 50-70% das leituras totais | Alta |
| 3.2 | Migrar feed de fan-out (push) para pull model | 5.000 writes/post ‚Üí 0 | Alta |
| 3.3 | Eliminar `events_card_preview` e `users_preview` (usar projections no read) | 2 cole√ß√µes inteiras + triggers | M√©dia |
| 3.4 | Migrar Cloud Functions v1 ‚Üí v2 (gen2) | 15-30% do custo Functions | M√©dia |
| 3.5 | Configurar lifecycle policies no Cloud Storage | Reduz storage em GB | Baixa |

### Fase 4 ‚Äî Migra√ß√£o para API Pr√≥pria (3-6 meses)

Conforme documentado em `PLANO_MIGRACAO_API_PROPRIA.md`:
- Manter no Firebase: **apenas Auth + Chat** (3 cole√ß√µes, ~7 streams)
- Migrar tudo para: **API NestJS + PostgreSQL (PostGIS)**
- Economia estimada: **85-90% do custo Firestore**
- Elimina√ß√£o de **~60 Cloud Functions** (restam apenas 3 de chat)

---

## Parte 6 ‚Äî M√©tricas para Monitoramento

### Dashboard Recomendado (GCP Monitoring)

```
M√©tricas essenciais:
1. Firestore reads/writes por cole√ß√£o por hora
2. Cloud Functions invocations por fun√ß√£o por hora
3. Cloud Functions execution time (p99) por fun√ß√£o
4. Cloud Storage egress (bytes) por bucket/path
5. Cloud Run instance count e billable time
6. Cloud Functions active instances (App Engine billing)
```

### Comandos de Verifica√ß√£o

```bash
# Verificar se h√° App Engine default service ativo
gcloud app versions list --project=partiu-479902

# Verificar custos por servi√ßo
gcloud billing accounts list
gcloud alpha billing budgets list

# Verificar Cloud Functions deployadas
firebase functions:list --project=partiu-479902

# Verificar storage lifecycle rules
gsutil lifecycle get gs://partiu-479902.appspot.com

# Verificar Cloud Run instances  
gcloud run services describe partiu-websocket --region=us-central1

# Ver m√©tricas de Firestore
gcloud firestore operations list --project=partiu-479902
```

---

## Conclus√£o

O custo rotulado como "App Engine" vem quase inteiramente das **63 Cloud Functions v1** que executam na infraestrutura App Engine. As a√ß√µes de maior impacto imediato s√£o:

1. **Remover 11 fun√ß√µes de migration/debug** do deploy
2. **Reduzir frequ√™ncia dos cron jobs** (especialmente `syncRankingFilters`)
3. **Corrigir streams sem `.limit()`** no Flutter (especialmente mapa e notifications)
4. **Migrar para Cloud Functions v2** (billing por request, n√£o por instance-hour)

A soma dessas 4 a√ß√µes pode reduzir **30-50% do custo atual de "App Engine/Functions"** antes de qualquer migra√ß√£o para API pr√≥pria.

Para Cloud Storage, instalar **"Resize Images"** no Firebase + corrigir `NetworkImage` ‚Üí `CachedNetworkImage` pode reduzir **50-70% do custo de egress**.
